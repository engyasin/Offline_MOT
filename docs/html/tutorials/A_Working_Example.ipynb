{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Working Example\n",
    "\n",
    "This example shows :\n",
    "\n",
    "1 - how to run the program for some video\n",
    "\n",
    "2 - how to set some important parameters for the detection, fixing and post processing\n",
    "\n",
    "3 - how to show the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detection part\n",
    "The detection part is seprarted by loading a pretrained YOLOv4 network. The training should be done separately before running this tracking program.\n",
    "\n",
    "The library that load the structure of YOLO is part of the project (https://github.com/Tianxiaomo/pytorch-YOLOv4)\n",
    "\n",
    "Using other detection trained model is also possible, it just need to be loaded within the class `YoloDetector` with making corresponding changes to its methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load all the libraries shown below, the directory should be at the root of the project folder. If this is ran in the `docs` directory, then one can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE :To add path of the main files\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "Dir = os.getcwd().split('\\\\')[-1]\n",
    "\n",
    "if Dir =='docs':\n",
    "    sys.path.insert(1, '../offlinemot')\n",
    "elif Dir == 'Offline_MOT':\n",
    "    %cd 'docs'\n",
    "    sys.path.insert(1, 'offlinemot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection import YoloDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can take several input parameters, like the YOLO configuration file, the model file (in pytorch format) and lastly a flag to wheather to use GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convalution havn't activate linear\n",
      "convalution havn't activate linear\n",
      "convalution havn't activate linear\n"
     ]
    }
   ],
   "source": [
    "detector  = YoloDetector('../model/yolov4-obj.cfg','../model/Yolov4_epoch300.pth',use_cuda=False,namesfile='../model/obj.names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection is only performed every *N* frame. Where *N* is set according to `config.py` file. The following is a detailed description of all the paramers and its effects.\n",
    "\n",
    "**Note** A several testing with the parameters values maybe  needed for certain type of videos. Once a working set of values is found then all the similar videos can use the same set.\n",
    "\n",
    "\n",
    "## Config file content\n",
    "\n",
    "\n",
    "With this variable:  `detect_every_N`, the number of detections frequency is determined.\n",
    "If high accuracy is needed then a value of 1 is optimal.\n",
    "Bigger values are less accurate but faster to run.\n",
    "\n",
    "### Detection and general parameters\n",
    "\n",
    "Other paramters influencing the detection and the general settings are:\n",
    "\n",
    "```python\n",
    "    ### general paprmters\n",
    "    draw = False\n",
    "    detect_every_N = 2\n",
    "    missing_thresh = 0.8\n",
    "    use_cuda = False\n",
    "    resize_scale = 0.4\n",
    "    \n",
    "    ### Detection paramters\n",
    "    model_name = 'model/Yolov4_epoch300.pth'\n",
    "    model_config = 'model/yolov4-obj.cfg'\n",
    "    classes_file_name = 'model/obj.names'\n",
    "    detect_thresh = 0.3 #Yolo detection\n",
    "    # distance to the nearst match between detection and tracking\n",
    "    # output in pixels\n",
    "    dist_thresh = 25\n",
    "    size_thresh = 25\n",
    "    detect_scale = 4.0\n",
    "    \n",
    "```\n",
    "\n",
    "- The darwing flag `darw`, whether to draw or not.\n",
    "\n",
    "- The `missing_thresh`parameter is for determining when to delete an object if it keeps failing tracking and detetcion, it should be between 0 and 1, with 0 means never delete, and 1 delete on the first failure. A value of 0.9 means delete if 10% of result is failed, keep otherwise.\n",
    "\n",
    "- The `use_cuda` flag is whether to use GPU for detection or CPU.\n",
    "\n",
    "- The `resize_scale` is just for display (if `draw` flag is True), to determine how much the image should be rescaled.\n",
    "\n",
    "- The `model_name` is for the trained model file name\n",
    "\n",
    "- The `model_config` is for setting the parameters of the Yolo network, if a different structure is trained then a different file should be given here.\n",
    "\n",
    "- The `class_file_name` is a text file contining the names of the pridected classes from the detection network. \n",
    "\n",
    "\n",
    "- The `detect_thresh` is used to put a threshold on YOLO probabilty output for each detected class. The lower this value is the more detections it will give but with less accuracy. \n",
    "\n",
    "- The `dist_thresh` is for matching each detection with already detected object, it shouldn't be too big because it represent the minmum distance to match. Otherwise, False matching will be given.\n",
    "\n",
    "- The `size_thresh` is another distance but to the change in width and height between a detection and a nearby object. It is used because the same object from bird's eye view should have the same dimensions. \n",
    "\n",
    "- The `detect_scale` is for smaller objects detection. sometimes the drone is too high and the objects are small, so we need to *zoom in* to detect in a better way. one solution here is to detect in two levels: full scale image in the ordinary case and smaller proposed cropped areas. This parameter control how small these areas are. Higher values will make the areas bigger. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing and smoothing\n",
    "\n",
    "In `config.py` file the following are the parameters for the part of fixing the view. The first boolean is for whether to do fixing or not. It will slow the processing, so if the video is stationary without noise then no need for it.\n",
    "\n",
    "```python\n",
    "### fix view paramers\n",
    "do_fix = False\n",
    "fixing_dilation = 13\n",
    "min_matches     = 15\n",
    "```\n",
    "\n",
    "The part for doing the smoothing is also included in the same file as follows. The first boolean is whether to do the smoothing or not, the other two are related to Savitzky-Golay filter from `scipy` library.\n",
    "\n",
    "The last boolean flag `save_out_video` will save a new mp4 file in the `outputs`folder with the same name for the processed file if set to True.\n",
    "\n",
    "```python\n",
    "### Smoothing for post processing\n",
    "do_smooth   = True\n",
    "window_size = 7\n",
    "polydegree  = 3\n",
    "save_out_video = False\n",
    "```\n",
    "\n",
    "Other steps (along with smoothing) are included in the `postprocess.py` file, namely, the orientation calculation and interpolation of missing positions in the tracks. The orientation is calculated from the trajectory itself where the next point in the trajectory determines the current point heading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background subtraction parameters\n",
    "\n",
    "Many parmeters are avaliable for background subtraction, as follows,\n",
    "\n",
    "```python\n",
    "\n",
    "    ### background subtractor parameters\n",
    "    bgs_history = 5\n",
    "    bgs_threshold = 50\n",
    "    bgs_shadows = True\n",
    "    bgs_learning = 0.5\n",
    "    bgs_erosion_size = 3\n",
    "    bgs_min_area = 300\n",
    "    bgs_broder_margin =  0.45    # bigger would give boxes near the detected boxes with yolo\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "The most important of them are:\n",
    "\n",
    "- The `bgs_history` determines how many frames are used to calculate the background\n",
    "\n",
    "- The `bgs_threshold` is realted to the sensativaty of the subtraction , lower values will give more sensativity to movements\n",
    "\n",
    "- The `bgs_erosion_size` is the s√≠ze of the mask to perform erosion on the forground. higher values will give thiner objects\n",
    "\n",
    "- The `bgs_min_area` determines the minmum area of pixels that will be considered as object, otherwise it will be deleted.\n",
    "\n",
    "- THe `bgs_broder_margin` determines how mush overlapping is allowed between already detected objects and newly found forground. this number is considered a percentage of the detected object dimensions that objects are allowed to be in. For example a value of 0.5 will mean everywhere is allowed, because half of the dimension on all the objects means all the objects' area.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filering parameters\n",
    "\n",
    "Additionally, in the `config` file, there's parameters that will determine when to delete the detected objects.\n",
    "\n",
    "```python\n",
    "\n",
    "    ### Filtering Objects:\n",
    "    min_history = 100\n",
    "    overlap_thresh = 0.7\n",
    "```\n",
    "\n",
    "- The `min_hisory` is the minmum number of frames that the objects will start to be tested for percentage of correct tracking (with `missing_thresh`).\n",
    "\n",
    "- The `overlap_thresh` is for the minmum area percentage of overlapping between confirmed objects to delete one of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "To run the full program, you need to run the `main.py` script with the `-v` flag set to the name and directory of the video in the base directory of the project. \n",
    "\n",
    "Try to run the following command to see the result on the sample video:\n",
    "\n",
    "`python main.py -v docs\\sample.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to show result\n",
    "\n",
    "After running the program on your video, a txt file will be saved in the **outputs** directory with the same name as the provided video.\n",
    "\n",
    "*Note* if your videos are named with similar names (for example numbers), you should copy the last result to other folder before start tracking a new video, because any new file with similar name will overwrite the previous one without warning.\n",
    "\n",
    "\n",
    "If you want to show the result with angles and smoothing for the sample video above, you can run the command: \n",
    "\n",
    "`python show_results.py -v docs\\sample.mp4`\n",
    "\n",
    "The directory after the -v flag is the same as the one from which the video were processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output structure\n",
    "\n",
    "in the output text file, each line is structured as follows,\n",
    "\n",
    "*Frame_id* ---- \\[ top_left_x ----- top_left_y ----- width height\\] ----- *class_id* ----- *track_id* ----- *angel*\n",
    "\n",
    "Example of one line:\n",
    "\n",
    "¬¥39 [3748, 964, 169, 73] 2 5 138¬¥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
